# -*- coding: utf-8 -*-
"""Copy of UntitledQOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AR-wQ3HBKzaqXWxDTPW2iXZj_rRgBVzH
"""



"""# 1. Introduction

- Importing data: Clean, organize and preprocessing
- Train on base model (e.g. XGBoost, LGBoost)
- Save models for later use
- Evaluate on test sets
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install xarray
# !pip install netcdf4
# !pip install pytorch-metric-learning
# !pip install pytorch-tabnet
# !pip install xgboost tqdm
# !pip install shap

from google.colab import drive
drive.mount('/content/drive')

import xarray as xr

lp_ds = xr.open_dataset("/content/drive/MyDrive/QoT/lightpath_dataset.nc")
data = lp_ds.data.to_pandas()
target = lp_ds.target.to_pandas()

data.columns

target

target = target['class']

target.value_counts()

data = data.drop(["conn_id", "src_id", "dst_id"], axis = 1)

import pandas as pd

def filter_dataframe_and_target(df, target):
    # Build the mask for rows to keep
    mask = (
        (df['num_links'] > 2) &
        ((df['num_spans'] > 6) | (df['num_spans'] < 18)) &
        (df['mod_order'] != 16.0) &
        (df['lp_linerate'] != 168.0) &
        (df['max_mod_order_left'] != 0) &
        (~df['min_lp_linerate_left'].isin([280, 336])) &
        (df['max_lp_linerate_left'] != 0) &
        (df['min_lp_linerate_right'] != 336)
    )

    # Apply mask
    filtered_df = df.loc[mask].reset_index(drop=True)
    filtered_target = target.loc[mask].reset_index(drop=True)

    return filtered_df, filtered_target

# df is your feature dataframe
# target is your Series or array (must have same number of rows as df)

data, target = filter_dataframe_and_target(data, target)

from sklearn.model_selection import train_test_split

X_temp, X_test, y_temp, y_test = train_test_split(
    data, target, test_size=0.15, stratify=target, random_state=42
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.15, stratify=y_temp, random_state=42
)

print(X_train.shape)
print(X_val.shape)
print(X_test.shape)

print(y_train.shape)
print(y_val.shape)
print(y_test.shape)

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
import pandas as pd

categorical_columns = ['mod_order','lp_linerate','conn_linerate','src_degree','dst_degree', \
                       'min_mod_order_left', 'max_mod_order_left', 'min_mod_order_right',  \
                       'max_mod_order_right', 'min_lp_linerate_left', 'max_lp_linerate_left', 'min_lp_linerate_right', \
                       'max_lp_linerate_right'
                       ]

numerical_columns = [col for col in data.columns if col not in categorical_columns]

# Create the pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_columns),
        # categoricals are untouched
    ],
    remainder='passthrough'
)

# Fit on training data only
preprocessor.fit_transform(X_train)

# Transform all sets
X_train_scaled = pd.DataFrame(
    preprocessor.fit_transform(X_train),
    columns=X_train.columns,
    index=X_train.index
)

X_val_scaled = pd.DataFrame(
    preprocessor.fit_transform(X_val),
    columns=X_val.columns,
    index=X_val.index
)

X_test_scaled = pd.DataFrame(
    preprocessor.fit_transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)

import xgboost as xgb
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report
from tqdm import tqdm

dtrain = xgb.DMatrix(X_train_scaled, label=y_train)
dval = xgb.DMatrix(X_val_scaled, label=y_val)
dtest = xgb.DMatrix(X_test_scaled, label=y_test)

params = {
    'objective': 'binary:logistic',  # Binary classification
    'eval_metric': 'auc',            # Primary metric = AUC
    'max_depth': 6,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'seed': 42,
    'verbosity': 0,                  # Silent logs, we manage progress ourselves
}

num_boost_round = 300
early_stopping_rounds = 20

print("ðŸ”µ Starting XGBoost training...")

evals_result = {}
watchlist = [(dtrain, 'train'), (dval, 'eval')]

# Create a tqdm progress bar
pbar = tqdm(total=num_boost_round, desc="Training Progress")

class TQDMCallback(xgb.callback.TrainingCallback):
    def after_iteration(self, model, epoch, evals_log):
        pbar.update(1)
        return False

# Train
model = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=num_boost_round,
    evals=watchlist,
    early_stopping_rounds=early_stopping_rounds,
    evals_result=evals_result,
    verbose_eval=False,
    callbacks=[TQDMCallback()]
)

pbar.close()
print("âœ… Training completed!")

model.save_model("/content/drive/MyDrive/QoT/xgboost_model.json")

print(f"Training Score: {model.best_score}")

print("\nðŸ”µ Evaluating on Test Set (data1)...")

# Predict
y_pred_prob = model.predict(dtest)
y_pred = (y_pred_prob > 0.5).astype(int)

# Compute Metrics
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)

# Print Metrics
print("\nðŸ“Š Performance on Test Set:")
print(f"Accuracy      : {acc:.4f}")
print(f"F1-Score      : {f1:.4f}")
print(f"Precision     : {precision:.4f}")
print(f"Recall        : {recall:.4f}")
print(f"ROC-AUC Score : {roc_auc:.4f}")

# Full Classification Report
print("\nðŸ“„ Detailed Classification Report:")
print(classification_report(y_test, y_pred, digits=4))

import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report
from tqdm import tqdm
from lightgbm.callback import early_stopping, log_evaluation

dtrain = lgb.Dataset(X_train_scaled, label=y_train)
dval = lgb.Dataset(X_val_scaled, label=y_val)
dtest = lgb.Dataset(X_test_scaled, label=y_test, reference=dtrain)

params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'learning_rate': 0.05,
    'num_leaves': 31,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'seed': 42,
    'verbosity': -1  # silent logs
}

num_boost_round = 300
early_stopping_rounds = 20

print("ðŸ”µ Starting LightGBM training...")

# Progress bar
pbar = tqdm(total=num_boost_round, desc="Training Progress")

# Custom callback to update tqdm
def lgb_tqdm_callback(env):
    pbar.update(1)

# Train the model
model = lgb.train(
    params=params,
    train_set=dtrain,
    valid_sets=[dtrain, dval],
    valid_names=['train', 'valid'],
    num_boost_round=num_boost_round,
    callbacks=[
        early_stopping(stopping_rounds=early_stopping_rounds),
        log_evaluation(period=10),
        lgb_tqdm_callback
    ]
)

pbar.close()
print("âœ… Training completed!")

model.save_model("/content/drive/MyDrive/QoT/lightgbm_model.json")

print("\nðŸ”µ Evaluating on Test Set (data1) ...")

# Predict
y_pred_prob = model.predict(X_test_scaled)
y_pred = (y_pred_prob > 0.5).astype(int)

# Compute Metrics
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)

# Print Metrics
print("\nðŸ“Š Performance on Test Set:")
print(f"Accuracy      : {acc:.4f}")
print(f"F1-Score      : {f1:.4f}")
print(f"Precision     : {precision:.4f}")
print(f"Recall        : {recall:.4f}")
print(f"ROC-AUC Score : {roc_auc:.4f}")

# Full Classification Report
print("\nðŸ“„ Detailed Classification Report:")
print(classification_report(y_test, y_pred, digits=4))

lp_ds2 = xr.open_dataset("/content/drive/MyDrive/QoT/lightpath_dataset_2.nc")
data2 = lp_ds2.data.to_pandas()
target2 = lp_ds2.target.to_pandas()

from sklearn.utils import shuffle
data2 = data2.drop(["conn_id", "src_id", "dst_id"], axis = 1)
target2 = target2['class']
data2, target2 = shuffle(data2, target2, random_state=42)
data2 = data2.reset_index(drop=True)
target2 = target2.reset_index(drop=True)

data2, target2 = filter_dataframe_and_target(data2, target2)

data2_scaled = pd.DataFrame(
    preprocessor.fit_transform(data2),
    columns=data2.columns,
)

data2.shape , target2.shape

subset_sample = data2_scaled.sample(frac=0.05, random_state=42)
subset_target = target[subset_sample.index]

subset_sample.shape, subset_target.shape

data2_95 = data2_scaled.drop(index=subset_sample.index)
target2_95 = target2.drop(index=subset_sample.index)

assert len(data2_scaled) == len(target2)
assert len(data2_scaled) == (len(subset_sample)+len(data2_95))



from xgboost import XGBClassifier

model = XGBClassifier()
model.load_model('/content/drive/MyDrive/QoT/xgboost_model.json')

subset_indices = subset_sample.index
# Exclude training subset
test_data = data2_scaled.drop(index=subset_indices)
test_target = target2.drop(index=subset_indices)

dtest_xgb = xgb.DMatrix(test_data)

y_pred_xgb_prob = model.predict_proba(test_data)[:, 1]  # Probability of class 1
y_pred_xgb = (y_pred_xgb_prob > 0.5).astype(int)

def evaluate_model(y_true, y_pred, y_pred_prob, model_name):
    print(f"\nðŸ“Š Results for {model_name}:")
    print(f"Accuracy      : {accuracy_score(y_true, y_pred):.4f}")
    print(f"F1-Score      : {f1_score(y_true, y_pred):.4f}")
    print(f"Precision     : {precision_score(y_true, y_pred):.4f}")
    print(f"Recall        : {recall_score(y_true, y_pred):.4f}")
    print(f"ROC-AUC Score : {roc_auc_score(y_true, y_pred_prob):.4f}")
    print("\nðŸ“„ Classification Report:")
    print(classification_report(y_true, y_pred, digits=4))

# Evaluate XGBoost
evaluate_model(test_target, y_pred_xgb, y_pred_xgb_prob, "XGBoost")

dtrain = xgb.DMatrix(subset_sample, label=subset_target)

params = {
    'objective': 'binary:logistic',  # Binary classification
    'eval_metric': 'auc',            # Primary metric = AUC
    'max_depth': 6,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'seed': 42,
    'verbosity': 0,                  # Silent logs, we manage progress ourselves
}

num_boost_round = 300
early_stopping_rounds = 20

print("ðŸ”µ Starting XGBoost training...")

evals_result = {}
watchlist = [(dtrain, 'train')]

# Create a tqdm progress bar
pbar = tqdm(total=num_boost_round, desc="Training Progress")

class TQDMCallback(xgb.callback.TrainingCallback):
    def after_iteration(self, model, epoch, evals_log):
        pbar.update(1)
        return False

model = XGBClassifier()
model.load_model('/content/drive/MyDrive/QoT/xgboost_model.json')


# Train
model_ft = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=num_boost_round,
    evals=watchlist,
    early_stopping_rounds=early_stopping_rounds,
    evals_result=evals_result,
    verbose_eval=False,
    callbacks=[TQDMCallback()],
    xgb_model='/content/drive/MyDrive/QoT/xgboost_model.json'
)

pbar.close()
print("âœ… Training completed!")

model_ft.save_model("/content/drive/MyDrive/QoT/xgboost_model_ft.json")

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report

subset_indices = subset_sample.index
# Exclude training subset
test_data = data2_scaled.drop(index=subset_indices)
test_target = target2.drop(index=subset_indices)

dtest_xgb = xgb.DMatrix(test_data)

y_pred_xgb_prob = model_ft.predict(dtest_xgb)  # Gives probability for class 1
y_pred_xgb = (y_pred_xgb_prob > 0.5).astype(int)

def evaluate_model(y_true, y_pred, y_pred_prob, model_name):
    print(f"\nðŸ“Š Results for {model_name}:")
    print(f"Accuracy      : {accuracy_score(y_true, y_pred):.4f}")
    print(f"F1-Score      : {f1_score(y_true, y_pred):.4f}")
    print(f"Precision     : {precision_score(y_true, y_pred):.4f}")
    print(f"Recall        : {recall_score(y_true, y_pred):.4f}")
    print(f"ROC-AUC Score : {roc_auc_score(y_true, y_pred_prob):.4f}")
    print("\nðŸ“„ Classification Report:")
    print(classification_report(y_true, y_pred, digits=4))

# Evaluate XGBoost
evaluate_model(test_target, y_pred_xgb, y_pred_xgb_prob, "XGBoost")

"""_____"""

combined_data = pd.concat([data, data2], ignore_index=True)

categorical_columns = ['mod_order','lp_linerate','conn_linerate','src_degree','dst_degree', \
                       'min_mod_order_left', 'max_mod_order_left', 'min_mod_order_right',  \
                       'max_mod_order_right', 'min_lp_linerate_left', 'max_lp_linerate_left', 'min_lp_linerate_right', \
                       'max_lp_linerate_right'
                       ]

category_sizes = {
    col: combined_data[col].nunique()
    for col in categorical_columns
}

vocab = {}
for col in categorical_columns:
    unique_vals = sorted(combined_data[col].dropna().unique())
    vocab[col] = {val: idx + 1 for idx, val in enumerate(unique_vals)}  # reserve 0 for <UNK>


num_numeric = len(combined_data.columns) - len(category_sizes)

def encode_column(series, mapping):
    return series.map(lambda val: mapping.get(val, 0)).fillna(0).astype(int)

for col in categorical_columns:
    X_train_scaled[col] = encode_column(X_train_scaled[col], vocab[col])
    X_test_scaled[col] = encode_column(X_test_scaled[col], vocab[col])
    X_val_scaled[col] = encode_column(X_val_scaled[col], vocab[col])
    data2_scaled[col] = encode_column(data2_scaled[col], vocab[col])
    subset_sample[col] = encode_column(subset_sample[col], vocab[col])
    data2_95[col] = encode_column(data2_95[col], vocab[col])

category_sizes = {col: len(vocab[col]) for col in categorical_columns}

category_sizes

import random
def set_global_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)

import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from pytorch_metric_learning.samplers import MPerClassSampler
import numpy as np
import random
torch.backends.cudnn.benchmark = True

class TabularDataset(Dataset):
    def __init__(self, X, y, cat_cols, num_cols):
        self.num_data = torch.as_tensor(X[num_cols].values, dtype=torch.float32)
        self.cat_data = torch.as_tensor(X[cat_cols].values, dtype=torch.long)
        # self.X = torch.as_tensor(X.values, dtype=torch.float32)
        self.y = torch.as_tensor(y.values, dtype=torch.long)

    def __len__(self):
        return len(self.num_data)


    def __getitem__(self, idx):
        return self.num_data[idx], self.cat_data[idx], self.y[idx]

def get_dataloader(X, y,cat_cols, num_cols, batch_size, sampler_type="m_per_class", m_per_class=4, shuffle=True, seed=42, num_workers=12, prefetch_factor=4):
    """
    Build a DataLoader with flexible sampling:
    - "m_per_class": Balanced batch with m samples per class
    - "class_balanced": Weighted random sampling inversely proportional to class frequencies
    - "random": Standard random shuffle
    Args:
        seed: random seed for any sampling randomness
    """
    dataset = TabularDataset(X, y, cat_cols, num_cols)
    sampler = None

    if isinstance(y, torch.Tensor):
        y_np = y.cpu().numpy()
    elif hasattr(y, "values"):  # Pandas Series
        y_np = y.values
    else:
        y_np = np.array(y)

    y_np = y_np.astype(int)

    if sampler_type == "m_per_class":
        # M samples per class in each batch
        sampler = MPerClassSampler(
            labels=y_np.tolist(),
            m=m_per_class,
        )
        shuffle = False

    elif sampler_type == "class_balanced":
        # Weighted sampling based on inverse class frequency
        class_counts = np.bincount(y_np)
        class_weights = 1. / class_counts
        weights = torch.DoubleTensor([class_weights[label] for label in y_np])
        sampler = WeightedRandomSampler(
            weights=weights,
            num_samples=len(weights),
            replacement=True,
            generator=torch.Generator().manual_seed(seed)  # Important for reproducibility
        )
        shuffle = False

    elif sampler_type == "random":
        # Standard random shuffling
        sampler = None
        shuffle = True

    else:
        raise ValueError(f"Unknown sampler_type: {sampler_type}. Must be 'm_per_class', 'class_balanced', or 'random'.")

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        shuffle=shuffle if sampler is None else False,
        drop_last=True,  # Important for contrastive learning stability
        num_workers = 2,
        prefetch_factor = prefetch_factor,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False
    )

    return dataloader

## Val and Test
num_cols = numerical_columns
cat_cols = categorical_columns

val_loader_bs128 = get_dataloader(X_val_scaled, y_val, cat_cols=cat_cols , num_cols=num_cols,  batch_size=128, sampler_type="random", seed=42)
test_loader_bs128 = get_dataloader(X_test_scaled, y_test, cat_cols=cat_cols , num_cols=num_cols,  batch_size=128, sampler_type="random", seed=42)

val_loader_bs256 = get_dataloader(X_val_scaled, y_val, cat_cols=cat_cols , num_cols=num_cols,  batch_size=256, sampler_type="random", seed=42)
test_loader_bs256 = get_dataloader(X_test_scaled, y_test, cat_cols=cat_cols , num_cols=num_cols,  batch_size=256, sampler_type="random", seed=42)

test_data2_loader_bs128 = get_dataloader(data2_95, target2_95, cat_cols=cat_cols , num_cols=num_cols,  batch_size=128, sampler_type="random", seed=42)
test_data2_loader_bs256 = get_dataloader(data2_95, target2_95, cat_cols=cat_cols , num_cols=num_cols,  batch_size=256, sampler_type="random", seed=42)

sample_data2_loader_bs128 = get_dataloader(subset_sample, subset_target, cat_cols=cat_cols , num_cols=num_cols,  batch_size=128, sampler_type="random", seed=42)
sample_data2_loader_bs256 = get_dataloader(subset_sample, subset_target, cat_cols=cat_cols , num_cols=num_cols,  batch_size=256, sampler_type="random", seed=42)

## MPerClass

# MPerClass balanced loader bs 128 m_per_class 8
train_loader_mc_bs128_m8 = get_dataloader(X_train_scaled, y_train, cat_cols=cat_cols , num_cols=num_cols, batch_size=128, sampler_type="m_per_class", m_per_class=8, seed=42)

# MPerClass balanced loader bs 256 m_per_class 16
train_loader_mc_bs256_m16 = get_dataloader(X_train_scaled, y_train, cat_cols=cat_cols , num_cols=num_cols,  batch_size=256, sampler_type="m_per_class", m_per_class=16, seed=42)

## ClassBalanced

# ClassBalanced loader bs 128
train_loader_cb_bs128 = get_dataloader(X_train_scaled, y_train, cat_cols=cat_cols , num_cols=num_cols, batch_size=128, sampler_type="class_balanced", m_per_class=None, seed=42)

# ClassBalanced loader bs 256
train_loader_cb_bs256 = get_dataloader(X_train_scaled, y_train, cat_cols=cat_cols , num_cols=num_cols, batch_size=256, sampler_type="class_balanced", m_per_class=None, seed=42)

import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualMLPBlock(nn.Module):
    """Twoâ€‘layer residual MLP block with BN, SiLU and dropout."""

    def __init__(self, dim: int, hidden: int = 256, dropout: float = 0.1):
        super().__init__()
        self.lin1 = nn.Linear(dim, hidden)
        self.lin2 = nn.Linear(hidden, dim)
        self.bn1 = nn.BatchNorm1d(hidden)
        self.bn2 = nn.BatchNorm1d(dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # (B, dim)
        residual = x
        x = F.silu(self.bn1(self.lin1(x)))
        x = self.dropout(self.bn2(self.lin2(x)))
        return F.silu(x + residual)

class TabularEncoder(nn.Module):
    """
    Numeric + (optional) categorical encoder â†’ â„“2-normalised embedding
    ------------------------------------------------------------------
    â€¢ category_sizes â€“ dict {column_name: n_unique_in_data1}
      (leave empty {} if you have no categoricals)
    â€¢ num_numeric     â€“ number of numerical columns
    """
    def __init__(
        self,
        category_sizes: dict[str, int],
        num_numeric: int,
        hidden_dim: int = 512,
        n_blocks: int = 3,
        out_dim: int = 128,
        dropout: float = 0.1,
        emb_dim: int = 32,                # base dimension for each cat column
    ):
        super().__init__()

        # ---------- categorical pathway ----------
        self.cat_embeddings = nn.ModuleDict()
        emb_out_dim = 0
        for col, size in category_sizes.items():
            dim = min(emb_dim, (size + 1) // 2)      # heuristic sizing
            self.cat_embeddings[col] = nn.Embedding(size + 1, dim)   # +1 = OOV
            emb_out_dim += dim

        # ---------- numeric pathway ----------
        self.num_bn   = nn.BatchNorm1d(num_numeric)
        self.num_proj = nn.Linear(num_numeric, hidden_dim // 2)

        # ---------- fusion + residual MLP ----------
        fusion_in = hidden_dim // 2 + emb_out_dim
        layers = [
            nn.Linear(fusion_in, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout),
        ]
        for _ in range(n_blocks):
            layers.append(ResidualMLPBlock(hidden_dim, hidden_dim, dropout))
        layers.append(nn.Linear(hidden_dim, out_dim))
        self.backbone = nn.Sequential(*layers)
        self.out_bn   = nn.BatchNorm1d(out_dim)

    # ------------------------------------------------------------------
    def forward(self, x_num: torch.Tensor, x_cat: torch.Tensor | None = None):
        """
        x_num : (B, num_numeric)  float32  â€“ already standard-scaled
        x_cat : (B, n_cat_cols)   long     â€“ indexes incl. OOV bucket
               *Pass None if you have no categorical columns.*
        """
        if x_cat is not None:
            cat_vecs = [
                self.cat_embeddings[col](x_cat[:, idx])
                for idx, col in enumerate(self.cat_embeddings.keys())
            ]
            cat_vec = torch.cat(cat_vecs, dim=1)
        else:
            cat_vec = None

        x_num = F.silu(self.num_proj(self.num_bn(x_num)))
        x = torch.cat([x_num, cat_vec], dim=1) if cat_vec is not None else x_num
        x = self.backbone(x)
        x = self.out_bn(x)
        return F.normalize(x, dim=1)


class ProjectionHead(nn.Module):
    def __init__(self, in_dim: int, proj_dim: int = 128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, proj_dim),
            nn.SiLU(),
            nn.Linear(proj_dim, proj_dim),
        )

    def forward(self, x):
        return F.normalize(self.mlp(x), dim=1)

class SCLModel(nn.Module):
    """Encoder + projection head wrapper."""

    def __init__(self, encoder: TabularEncoder, proj_dim: int = 128):
        super().__init__()
        self.encoder = encoder
        self.proj = ProjectionHead(encoder.out_bn.num_features, proj_dim)

    def forward(self, x):
        x_num, x_cat = x
        emb = self.encoder(x_num, x_cat)
        proj = self.proj(emb)
        return proj, emb

class ClassifierHead(nn.Module):
    def __init__(self, in_dim: int, hidden: int = 256, dropout: float = 0.2):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden),
            nn.BatchNorm1d(hidden),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden, 2),
        )

    def forward(self, x):
        return self.mlp(x)

encoder = TabularEncoder(category_sizes= category_sizes, \
        num_numeric = num_numeric, \
        hidden_dim = 512, \
        n_blocks = 3, \
        out_dim = 128, \
        dropout = 0.15, \
        emb_dim = 32)

scl = SCLModel(encoder, proj_dim=128)

classifier = ClassifierHead(in_dim=128, hidden=256, dropout=0.2)

# Testing

scl.eval()
classifier.eval()

x_num, x_cat, y = val_loader_bs128.dataset[0]
out = scl((x_num.unsqueeze(0), x_cat.unsqueeze(0)))
print(out[0].shape)  # proj shape â†’ (1, 128)
print(out[1].shape)  # emb shape  â†’ (1, 128)

out = classifier(out[0])
print(out.shape)  # classifier shape â†’ (1, 2)

import torch
from torch.cuda.amp import autocast, GradScaler
from tqdm.notebook import tqdm

class Trainer:
    def __init__(self, sclmodel, train_loader, val_loader, loss_fn, miner_fn=None, optimizer=None, scheduler=None, device=None, use_amp=True, save_path="best_model.pt", early_stopping_patience=10):
        self.sclmodel = sclmodel
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.loss_fn = loss_fn
        self.miner_fn = miner_fn
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.use_amp = use_amp  # Automatic Mixed Precision toggle
        self.scaler = GradScaler() if use_amp else None  # GradScaler for AMP
        self.save_path = save_path
        self.early_stopping_patience = early_stopping_patience


        if self.sclmodel is not None:
            self.sclmodel.to(self.device)

    def train(self, epochs=10, validate_every=1):
        train_losses, val_losses = [], []
        best_val_loss = float('inf')
        epochs_no_improve = 0

        print(f"\nðŸ”µ Starting Training for {epochs} epochs...")
        print(f"ðŸ”µ Encoder: {'Yes' if self.sclmodel is not None else 'No'} | Miner: {'Yes' if self.miner_fn is not None else 'No'}")
        print(f"ðŸ”µ Device: {self.device} | Mixed Precision: {'Yes' if self.use_amp else 'No'}\n")

        for epoch in range(epochs):
            if self.sclmodel is not None:
                self.sclmodel.train()
            epoch_loss = 0
            num_batches = 0

            loop = tqdm(self.train_loader, desc=f"ðŸ”„ Epoch [{epoch+1}/{epochs}] Training", leave=False)
            for x_num, x_cat, label in loop:
                x_num = x_num.to(self.device, non_blocking=True)
                x_cat = x_cat.to(self.device, non_blocking=True)
                batch_labels = label.to(self.device, non_blocking=True)

                self.optimizer.zero_grad()

                # --- Forward and loss computation inside autocast ---
                with autocast(enabled=self.use_amp):
                    if self.sclmodel is not None:
                        embeddings, _ = self.sclmodel((x_num, x_cat))
                    else:
                        embeddings = (x_num, x_cat)

                    if self.miner_fn:
                        pairs = self.miner_fn(embeddings, batch_labels)
                        loss = self.loss_fn(embeddings, batch_labels, pairs)
                    else:
                        loss = self.loss_fn(embeddings, batch_labels)

                # --- Backward ---
                if self.use_amp:
                    self.scaler.scale(loss).backward()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    self.optimizer.step()

                epoch_loss += loss.item()
                num_batches += 1

                loop.set_postfix(loss=loss.item())

            avg_epoch_loss = epoch_loss / num_batches
            avg_train_loss = epoch_loss / len(self.train_loader)
            train_losses.append(avg_epoch_loss)
            print(f"âœ… Epoch [{epoch+1}/{epochs}] | Avg Train Loss: {avg_epoch_loss:.6f}")


            current_lr = self.optimizer.param_groups[0]["lr"]
            print(f"âœ… Epoch {epoch+1}: Train Loss = {avg_train_loss:.6f} | LR = {current_lr:.6f}")

            if self.val_loader is not None:
              # --- Validation ---
              if (epoch + 1) % validate_every == 0:
                  val_loss = self.validate()
                  val_losses.append(val_loss)
                  print(f"ðŸ“ˆ Epoch [{epoch+1}/{epochs}] | Validation Loss: {val_loss:.6f}")

                  if val_loss < best_val_loss:
                      best_val_loss = val_loss
                      epochs_no_improve = 0
                      torch.save(self.sclmodel.state_dict(), self.save_path)
                      print(f"ðŸ’¾ Saved best model to {self.save_path}")
                  else:
                      epochs_no_improve += 1
                      print(f"â³ No improvement for {epochs_no_improve} epoch(s)")


                  if self.scheduler:
                      if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                          self.scheduler.step(val_loss)  # pass validation loss
                      else:
                          self.scheduler.step()


                  if epochs_no_improve >= self.early_stopping_patience:
                      print("â›” Early stopping triggered")
                      break

        torch.save(self.sclmodel.state_dict(), self.save_path)
        print(f"ðŸ’¾ Saved best model to {self.save_path}")

        return train_losses, val_losses

    def validate(self):
        if self.sclmodel is not None:
            self.sclmodel.eval()

        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            loop = tqdm(self.val_loader, desc="ðŸ§ª Validation", leave=False)
            for x_num, x_cat, label in loop:
                x_num = x_num.to(self.device, non_blocking=True)
                x_cat = x_cat.to(self.device, non_blocking=True)
                batch_labels = label.to(self.device, non_blocking=True)

                with autocast(enabled=self.use_amp):
                    if self.sclmodel is not None:
                        embeddings, _ = self.sclmodel((x_num, x_cat))
                    else:
                        embeddings = (x_num, x_cat)

                    loss = self.loss_fn(embeddings, batch_labels)

                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches
        return avg_loss

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score


def train_mlp_on_embeddings(encoder, classifier, train_loader, val_loader, embedding_dim, epochs=10, device=None,
                            early_stopping_patience=10, save_path="best_mlp.pt"):
    device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    best_val_loss = float('inf')

    # 1. Freeze encoder
    encoder.eval()
    for param in encoder.parameters():
        param.requires_grad = False
    encoder.to(device)



    # 2. Prepare new datasets: embeddings instead of raw features
    def encode_dataset(loader):
        embeddings_list = []
        labels_list = []
        with torch.no_grad():
            for x_num, x_cat, batch_labels in loader:
                x_num = x_num.to(device, non_blocking=True)
                x_cat = x_cat.to(device, non_blocking=True)
                batch_labels = batch_labels.to(device, non_blocking=True)

                embeddings, _ = encoder((x_num, x_cat))
                embeddings_list.append(embeddings.cpu())
                labels_list.append(batch_labels.cpu())
        return torch.cat(embeddings_list), torch.cat(labels_list)

    train_embeddings, train_labels = encode_dataset(train_loader)
    if val_loader is not None:
      val_embeddings, val_labels = encode_dataset(val_loader)

    train_dataset = TensorDataset(train_embeddings, train_labels)
    if val_loader is not None:
      val_dataset = TensorDataset(val_embeddings, val_labels)

    train_embed_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    if val_loader is not None:
      val_embed_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)

    # 3. Initialize small MLP
    classifier.to(device)
    optimizer = torch.optim.AdamW(classifier.parameters(), lr=1e-3)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)
    criterion = nn.CrossEntropyLoss()

    print("ðŸ”µ Training MLP classifier on frozen encoder embeddings...")

    for epoch in range(epochs):
        classifier.train()
        total_loss = 0
        for batch_x, batch_labels in train_embed_loader:
            batch_x = batch_x.to(device)
            batch_labels = batch_labels.to(device)

            preds = classifier(batch_x)
            loss = criterion(preds, batch_labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_embed_loader)

        # --- Validation
        if val_loader is not None:
          classifier.eval()
          all_preds = []
          all_true = []
          val_loss = 0
          with torch.no_grad():
              for batch_x, batch_labels in val_embed_loader:
                  batch_x = batch_x.to(device)
                  batch_labels = batch_labels.to(device)
                  preds = classifier(batch_x)

                  #?
                  loss = criterion(preds, batch_labels)
                  val_loss += loss.item()

                  pred_classes = torch.argmax(preds, dim=1)
                  all_preds.append(pred_classes.cpu())
                  all_true.append(batch_labels.cpu())

          val_loss /= len(val_embed_loader)
          y_pred = torch.cat(all_preds).numpy()
          y_true = torch.cat(all_true).numpy()

          acc = accuracy_score(y_true, y_pred)
          f1 = f1_score(y_true, y_pred)
          roc = roc_auc_score(y_true, y_pred)
          precision = precision_score(y_true, y_pred)
          recall = recall_score(y_true, y_pred)

          print(f"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Acc: {acc:.4f} | F1: {f1:.4f} | ROC-AUC: {roc:.4f}")

          scheduler.step(val_loss)

          # Early stopping check
          if val_loss < best_val_loss:
              best_val_loss = val_loss
              epochs_no_improve = 0
              torch.save(classifier.state_dict(), save_path)
              print(f"ðŸ’¾ Best model saved to {save_path}")
          else:
              epochs_no_improve += 1
              if epochs_no_improve >= early_stopping_patience:
                  print("â›” Early stopping triggered.")
                  break


    torch.save(classifier.state_dict(), save_path)
    print(f"ðŸ’¾ Best model saved to {save_path}")
    # Load best model before returning
    # classifier.load_state_dict(torch.load(save_path))
    return classifier

"""# TRAIN SCL FIRST, CLASSIFIER NEXT"""

# # Create dummy components for dry-run test
# from pytorch_metric_learning.losses import MultiSimilarityLoss
# from pytorch_metric_learning.miners import MultiSimilarityMiner
# import torch.nn as nn
# import torch.optim as optim

# # Assume sclmodel, train_loader, val_loader already defined
# loss_fn = MultiSimilarityLoss()
# miner = MultiSimilarityMiner()
# optimizer = optim.AdamW(scl.parameters(), lr=1e-3)
# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')

# trainer = Trainer(
#     sclmodel=scl,
#     train_loader=train_loader_mc_bs256_m16,
#     val_loader=val_loader_bs256,
#     loss_fn=loss_fn,
#     miner_fn=miner,
#     optimizer=optimizer,
#     scheduler=scheduler,
#     use_amp=True,
#     save_path="scl_model_MC_256.pt",
#     early_stopping_patience=7
# )

# train_losses, val_losses = trainer.train(epochs=70)

# Create dummy components for dry-run test
from pytorch_metric_learning.losses import MultiSimilarityLoss, AngularLoss, NTXentLoss, FastAPLoss, CosFaceLoss, IntraPairVarianceLoss, LargeMarginSoftmaxLoss, NormalizedSoftmaxLoss, ProxyAnchorLoss, MultipleLosses
from pytorch_metric_learning.miners import MultiSimilarityMiner, AngularMiner, HDCMiner, BatchEasyHardMiner
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, classification_report

msloss = MultiSimilarityLoss()
angularloss = AngularLoss()
ntxentloss = NTXentLoss()
fastaploss = FastAPLoss()
cosfaceloss = CosFaceLoss(2,128)

intrapair = IntraPairVarianceLoss()
lgsftmloss = LargeMarginSoftmaxLoss(2,128)
normsftmxloss = NormalizedSoftmaxLoss(2,128)
proxyanchor = ProxyAnchorLoss(2,128)

msminer = MultiSimilarityMiner()
angularminer = AngularMiner()
hdcminer = HDCMiner()
behminer = BatchEasyHardMiner()

multiloss = MultipleLosses([intrapair, lgsftmloss, normsftmxloss, proxyanchor] , miners=[msminer, None, None, None])

losses = [msloss, angularloss, ntxentloss, fastaploss, intrapair, multiloss]

miners = [msminer, angularminer, hdcminer, behminer]

lossnames = ["msloss", "angularloss", "ntxentloss", "fastaploss", "cosfaceloss", "multiloss"]
minernames = ["msminer", "angularminer", "hdcminer", "behminer"]

for minername, miner in zip(minernames, miners):
    for lossname, loss_fn in zip(lossnames, losses):

        print(f"Loss: {lossname}, Miner: {minername}")
        print('===================================')


        if lossname in ['msloss', 'angularloss', 'ntxentloss', 'fastaploss']:
            continue

        encoder = TabularEncoder(category_sizes= category_sizes, \
                num_numeric = num_numeric, \
                hidden_dim = 512, \
                n_blocks = 3, \
                out_dim = 128, \
                dropout = 0.15, \
                emb_dim = 32)

        scl = SCLModel(encoder, proj_dim=128)

        classifier = ClassifierHead(in_dim=128, hidden=256, dropout=0.2)

        optimizer = optim.AdamW(scl.parameters(), lr=1e-3)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')

        trainer = Trainer(
            sclmodel=scl,
            train_loader=train_loader_cb_bs128,
            val_loader=val_loader_bs128,
            loss_fn=loss_fn,
            miner_fn=miner,
            optimizer=optimizer,
            scheduler=scheduler,
            use_amp=True,
            save_path=f"/content/drive/MyDrive/QoT/scl_model_CB_128_{lossname}_{minername}.pt",
            early_stopping_patience=3
        )


        train_losses, val_losses = trainer.train(epochs=50, validate_every=3)

        print("\n Training of scl before FT done! \n")

        path_to_save = f"/content/drive/MyDrive/QoT/mlp_model_CB_128_{lossname}_{minername}.pt"

        trained_classifier = train_mlp_on_embeddings(
            encoder=scl,
            classifier=classifier,
            train_loader=train_loader_cb_bs128,
            val_loader=val_loader_bs128,
            embedding_dim=128,
            epochs=50,
            early_stopping_patience=4,
            save_path=path_to_save
        )

        print("\n Training of classifier before FT done! \n")

        # ##EVAL BEFORE FT
        # test_data2_loader_bs128.num_workers = 0

        # scl.eval()
        # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # scl.to(device)

        # all_embeddings = []
        # all_labels = []

        # with torch.no_grad():
        #     for x_num, x_cat, batch_labels in test_data2_loader_bs128 :
        #         x_num = x_num.to(device)
        #         x_cat = x_cat.to(device)
        #         batch_labels = batch_labels.to(device)

        #         embeddings, _ = scl((x_num, x_cat))
        #         all_embeddings.append(embeddings.cpu())
        #         all_labels.append(batch_labels.cpu())

        # X_data2_embeddings = torch.cat(all_embeddings)
        # y_data2_true = torch.cat(all_labels)

        # classifier.eval()
        # classifier.to(device)

        # y_pred_classes = []
        # with torch.no_grad():
        #     for i in range(0, len(X_data2_embeddings), 128):
        #         batch_embeddings = X_data2_embeddings[i:i+128].to(device)
        #         preds = classifier(batch_embeddings)
        #         pred_classes = torch.argmax(preds, dim=1)
        #         y_pred_classes.append(pred_classes.cpu())

        # y_pred_classes = torch.cat(y_pred_classes)

        # y_true = y_data2_true.numpy()
        # y_pred = y_pred_classes.numpy()

        # acc = accuracy_score(y_true, y_pred)
        # f1 = f1_score(y_true, y_pred)
        # roc = roc_auc_score(y_true, y_pred)
        # precision = precision_score(y_true, y_pred)
        # recall = recall_score(y_true, y_pred)

        # print("ðŸ“Š Evaluation Results on Data2:")
        # print(f"Accuracy      : {acc:.4f}")
        # print(f"F1-Score      : {f1:.4f}")
        # print(f"Precision     : {precision:.4f}")
        # print(f"Recall        : {recall:.4f}")
        # print(f"ROC-AUC Score : {roc:.4f}")

        # print("\nðŸ“„ Detailed Classification Report:")
        # print(classification_report(y_true, y_pred, digits=4))

        # test_data2_loader_bs128.num_workers = 2
        # # FT
        # tab_enc = TabularEncoder(category_sizes= category_sizes, \
        #         num_numeric = num_numeric, \
        #         hidden_dim = 512, \
        #         n_blocks = 3, \
        #         out_dim = 128, \
        #         dropout = 0.15, \
        #         emb_dim = 32)


        # scl_model = SCLModel(tab_enc, proj_dim=128)

        # path_to_save = f"/content/drive/MyDrive/QoT/scl_model_CB_128_{lossname}_{minername}.pt"

        # scl_model.load_state_dict(torch.load(path_to_save))

        # optimizer = optim.AdamW(scl_model.parameters(), lr=1e-3)
        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')

        # path_to_save = f"/content/drive/MyDrive/QoT/scl_model_MC_128_{lossname}_{minername}_ft.pt"

        # trainer = Trainer(
        #     sclmodel=scl_model,
        #     train_loader=sample_data2_loader_bs128,
        #     val_loader=None,
        #     loss_fn=loss_fn,
        #     miner_fn=miner,
        #     optimizer=optimizer,
        #     scheduler=scheduler,
        #     use_amp=True,
        #     save_path=path_to_save,
        #     early_stopping_patience=4
        # )

        # train_losses, val_losses = trainer.train(epochs=50, validate_every=3)
        # print("\n Training of scl in FT mode done! \n")

        # path_to_save = f"/content/drive/MyDrive/QoT/mlp_model_CB_128_{lossname}_{minername}.pt"

        # mlp = ClassifierHead(in_dim=128, hidden=256, dropout=0.2)
        # mlp.load_state_dict(torch.load(path_to_save))

        # path_to_save = f"/content/drive/MyDrive/QoT/mlp_model_CB_128_{lossname}_{minername}_ft.pt"

        # trained_classifier = train_mlp_on_embeddings(
        #     encoder=scl_model,
        #     classifier=mlp,
        #     train_loader=sample_data2_loader_bs128,
        #     val_loader=None,
        #     embedding_dim=128,
        #     epochs=65,
        #     early_stopping_patience=4,
        #     save_path=path_to_save
        # )

        # print("\n Training of classifier in FT mode done! \n")
        # test_data2_loader_bs128.num_workers = 0

        # scl.eval()
        # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # scl.to(device)

        # all_embeddings = []
        # all_labels = []

        # with torch.no_grad():
        #     for x_num, x_cat, batch_labels in test_data2_loader_bs128 :
        #         x_num = x_num.to(device)
        #         x_cat = x_cat.to(device)
        #         batch_labels = batch_labels.to(device)

        #         embeddings, _ = scl((x_num, x_cat))
        #         all_embeddings.append(embeddings.cpu())
        #         all_labels.append(batch_labels.cpu())

        # X_data2_embeddings = torch.cat(all_embeddings)
        # y_data2_true = torch.cat(all_labels)

        # classifier.eval()
        # classifier.to(device)

        # y_pred_classes = []
        # with torch.no_grad():
        #     for i in range(0, len(X_data2_embeddings), 128):
        #         batch_embeddings = X_data2_embeddings[i:i+128].to(device)
        #         preds = classifier(batch_embeddings)
        #         pred_classes = torch.argmax(preds, dim=1)
        #         y_pred_classes.append(pred_classes.cpu())

        # y_pred_classes = torch.cat(y_pred_classes)

        # y_true = y_data2_true.numpy()
        # y_pred = y_pred_classes.numpy()

        # acc = accuracy_score(y_true, y_pred)
        # f1 = f1_score(y_true, y_pred)
        # roc = roc_auc_score(y_true, y_pred)
        # precision = precision_score(y_true, y_pred)
        # recall = recall_score(y_true, y_pred)

        # print("ðŸ“Š Evaluation Results on Data2:")
        # print(f"Accuracy      : {acc:.4f}")
        # print(f"F1-Score      : {f1:.4f}")
        # print(f"Precision     : {precision:.4f}")
        # print(f"Recall        : {recall:.4f}")
        # print(f"ROC-AUC Score : {roc:.4f}")

        # print("\nðŸ“„ Detailed Classification Report:")
        # print(classification_report(y_true, y_pred, digits=4))

msloss = MultiSimilarityLoss()
angularloss = AngularLoss()
ntxentloss = NTXentLoss()
fastaploss = FastAPLoss()
cosfaceloss = CosFaceLoss(2,128)

intrapair = IntraPairVarianceLoss()
lgsftmloss = LargeMarginSoftmaxLoss(2,128)
normsftmxloss = NormalizedSoftmaxLoss(2,128)
proxyanchor = ProxyAnchorLoss(2,128)

msminer = MultiSimilarityMiner()
angularminer = AngularMiner()
hdcminer = HDCMiner()
behminer = BatchEasyHardMiner()


losses = [msloss, angularloss, ntxentloss, fastaploss, cosfaceloss]

miners = [msminer, angularminer, hdcminer, behminer]

lossnames = ["msloss", "angularloss", "ntxentloss", "fastaploss", "cosfaceloss", "multiloss"]
minernames = ["msminer", "angularminer", "hdcminer", "behminer"]

test_data2_loader_bs128.num_workers = 0
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
for lossname, loss_fn in zip(lossnames, losses):
    for minername, miner in zip(minernames, miners):
        if lossname == "ntxentloss" and minername == "angularminer":
            continue

        print(f"Loss: {lossname}, Miner: {minername}")
        print('===================================')



        encoder = TabularEncoder(category_sizes= category_sizes, \
                num_numeric = num_numeric, \
                hidden_dim = 512, \
                n_blocks = 3, \
                out_dim = 128, \
                dropout = 0.15, \
                emb_dim = 32)

        scl = SCLModel(encoder, proj_dim=128)
        classifier = ClassifierHead(in_dim=128, hidden=256, dropout=0.2)

        scl.load_state_dict(torch.load(f"/content/drive/MyDrive/QoT/scl_model_CB_128_{lossname}_{minername}.pt"))
        classifier.load_state_dict(torch.load(f"/content/drive/MyDrive/QoT/mlp_model_CB_128_{lossname}_{minername}.pt"))

        # ##EVAL BEFORE FT
        scl.to(device)
        scl.eval()
        classifier.to(device)
        classifier.eval()
        all_embeddings = []
        all_labels = []

        with torch.no_grad():
            for x_num, x_cat, batch_labels in test_data2_loader_bs256 :
                x_num = x_num.to(device)
                x_cat = x_cat.to(device)
                batch_labels = batch_labels.to(device)

                embeddings, _ = scl((x_num, x_cat))
                all_embeddings.append(embeddings.cpu())
                all_labels.append(batch_labels.cpu())

        X_data2_embeddings = torch.cat(all_embeddings)
        y_data2_true = torch.cat(all_labels)

        y_pred_classes = []
        with torch.no_grad():
            for i in range(0, len(X_data2_embeddings), 128):
                batch_embeddings = X_data2_embeddings[i:i+128].to(device)
                preds = classifier(batch_embeddings)
                pred_classes = torch.argmax(preds, dim=1)
                y_pred_classes.append(pred_classes.cpu())

        y_pred_classes = torch.cat(y_pred_classes)

        y_true = y_data2_true.numpy()
        y_pred = y_pred_classes.numpy()

        acc = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        roc = roc_auc_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)

        print("ðŸ“Š Evaluation Results on Data2:")
        print(f"Accuracy      : {acc:.4f}")
        print(f"F1-Score      : {f1:.4f}")
        print(f"Precision     : {precision:.4f}")
        print(f"Recall        : {recall:.4f}")
        print(f"ROC-AUC Score : {roc:.4f}")

        print("\nðŸ“„ Detailed Classification Report:")
        print(classification_report(y_true, y_pred, digits=4))

import torch
from torch.utils.data import DataLoader, Subset, random_split

def shard_dataloader(dataloader, num_shards=4, seed=42):
    # Get the dataset from the existing DataLoader
    dataset = dataloader.dataset
    total_len = len(dataset)

    # Compute lengths of each shard
    shard_len = total_len // num_shards
    lengths = [shard_len] * num_shards
    # Distribute the remainder if total_len is not divisible by num_shards
    for i in range(total_len % num_shards):
        lengths[i] += 1

    # Set seed and split
    generator = torch.Generator().manual_seed(seed)
    subsets = random_split(dataset, lengths, generator=generator)

    # Create new dataloaders for each shard
    shard_loaders = [DataLoader(subset, batch_size=dataloader.batch_size,
                                shuffle=False, num_workers=dataloader.num_workers,
                                pin_memory=getattr(dataloader, 'pin_memory', False))
                     for subset in subsets]

    return shard_loaders

shard_loaders = shard_dataloader(test_data2_loader_bs128, num_shards=4, seed=123)

# Evaluate your model on each shard
for i, shard_loader in enumerate(shard_loaders):
    print("########################===============================#####################")
    print(f"Evaluating on shard {i+1}")
    print("########################===============================#####################")
    for lossname, loss_fn in zip(lossnames, losses):
      for minername, miner in zip(minernames, miners):
          if lossname == "ntxentloss" and minername == "angularminer":
              continue

          print(f"Loss: {lossname}, Miner: {minername}")
          print('===================================')

          encoder = TabularEncoder(category_sizes= category_sizes, \
                  num_numeric = num_numeric, \
                  hidden_dim = 512, \
                  n_blocks = 3, \
                  out_dim = 128, \
                  dropout = 0.15, \
                  emb_dim = 32)

          scl = SCLModel(encoder, proj_dim=128)
          classifier = ClassifierHead(in_dim=128, hidden=256, dropout=0.2)

          scl.load_state_dict(torch.load(f"/content/drive/MyDrive/QoT/scl_model_MC_256_{lossname}_{minername}.pt"))
          classifier.load_state_dict(torch.load(f"/content/drive/MyDrive/QoT/mlp_model_MC_256_{lossname}_{minername}.pt"))

          # ##EVAL BEFORE FT
          scl.to(device)
          scl.eval()
          classifier.to(device)
          classifier.eval()
          all_embeddings = []
          all_labels = []

          with torch.no_grad():
              for x_num, x_cat, batch_labels in test_data2_loader_bs256 :
                  x_num = x_num.to(device)
                  x_cat = x_cat.to(device)
                  batch_labels = batch_labels.to(device)

                  embeddings, _ = scl((x_num, x_cat))
                  all_embeddings.append(embeddings.cpu())
                  all_labels.append(batch_labels.cpu())

          X_data2_embeddings = torch.cat(all_embeddings)
          y_data2_true = torch.cat(all_labels)

          y_pred_classes = []
          with torch.no_grad():
              for i in range(0, len(X_data2_embeddings), 256):
                  batch_embeddings = X_data2_embeddings[i:i+256].to(device)
                  preds = classifier(batch_embeddings)
                  pred_classes = torch.argmax(preds, dim=1)
                  y_pred_classes.append(pred_classes.cpu())

          y_pred_classes = torch.cat(y_pred_classes)

          y_true = y_data2_true.numpy()
          y_pred = y_pred_classes.numpy()

          acc = accuracy_score(y_true, y_pred)
          f1 = f1_score(y_true, y_pred)
          roc = roc_auc_score(y_true, y_pred)
          precision = precision_score(y_true, y_pred)
          recall = recall_score(y_true, y_pred)

          print("ðŸ“Š Evaluation Results on Data2:")
          print(f"Accuracy      : {acc:.4f}")
          print(f"F1-Score      : {f1:.4f}")
          print(f"Precision     : {precision:.4f}")
          print(f"Recall        : {recall:.4f}")
          print(f"ROC-AUC Score : {roc:.4f}")

          print("\nðŸ“„ Detailed Classification Report:")
          print(classification_report(y_true, y_pred, digits=4))

msloss = MultiSimilarityLoss()
angularloss = AngularLoss()
ntxentloss = NTXentLoss()
fastaploss = FastAPLoss()
cosfaceloss = CosFaceLoss(2,128)

intrapair = IntraPairVarianceLoss()
lgsftmloss = LargeMarginSoftmaxLoss(2,128)
normsftmxloss = NormalizedSoftmaxLoss(2,128)
proxyanchor = ProxyAnchorLoss(2,128)

msminer = MultiSimilarityMiner()
angularminer = AngularMiner()
hdcminer = HDCMiner()
behminer = BatchEasyHardMiner()


losses = [msloss, angularloss, ntxentloss, fastaploss, cosfaceloss]

miners = [msminer, angularminer, hdcminer, behminer]

lossnames = ["msloss", "angularloss", "ntxentloss", "fastaploss", "cosfaceloss"]
minernames = ["msminer", "angularminer", "hdcminer", "behminer"]

test_data2_loader_bs256.num_workers = 0
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
for lossname, loss_fn in zip(lossnames, losses):
    for minername, miner in zip(minernames, miners):
        if lossname == "ntxentloss" and minername == "angularminer":
            continue

        if lossname == "angularloss" and minername == "msminer":
            continue

        print(f"Loss: {lossname}, Miner: {minername}")
        print('===================================')
        # FT
        tab_enc = TabularEncoder(category_sizes= category_sizes, \
                num_numeric = num_numeric, \
                hidden_dim = 512, \
                n_blocks = 3, \
                out_dim = 128, \
                dropout = 0.15, \
                emb_dim = 32)


        scl_model = SCLModel(tab_enc, proj_dim=128)

        path_to_save = f"/content/drive/MyDrive/QoT/scl_model_CB_128_{lossname}_{minername}.pt"

        scl_model.load_state_dict(torch.load(path_to_save))

        optimizer = optim.AdamW(scl_model.parameters(), lr=1e-3)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')

        path_to_save = f"/content/drive/MyDrive/QoT/scl_model_CB_128_{lossname}_{minername}_ft.pt"

        trainer = Trainer(
            sclmodel=scl_model,
            train_loader=sample_data2_loader_bs128,
            val_loader=None,
            loss_fn=loss_fn,
            miner_fn=miner,
            optimizer=optimizer,
            scheduler=scheduler,
            use_amp=True,
            save_path=f"/content/drive/MyDrive/QoT/scl_model_CB_128_{lossname}_{minername}_ft.pt",
            early_stopping_patience=4
        )

        train_losses, val_losses = trainer.train(epochs=75, validate_every=3)

        print("\n Training SCL in FT mode Done \n")

        path_to_save = f"/content/drive/MyDrive/QoT/mlp_model_CB_128_{lossname}_{minername}.pt"

        mlp = ClassifierHead(in_dim=128, hidden=256, dropout=0.2)
        mlp.load_state_dict(torch.load(path_to_save))

        path_to_save = f"/content/drive/MyDrive/QoT/mlp_model_CB_128_{lossname}_{minername}_ft.pt"

        trained_classifier = train_mlp_on_embeddings(
            encoder=scl_model,
            classifier=mlp,
            train_loader=sample_data2_loader_bs128,
            val_loader=None,
            embedding_dim=128,
            epochs=75,
            early_stopping_patience=4,
            save_path=path_to_save
        )

        print("\n Training Classifier in FT mode Done \n")

msloss = MultiSimilarityLoss()
angularloss = AngularLoss()
ntxentloss = NTXentLoss()
fastaploss = FastAPLoss()
cosfaceloss = CosFaceLoss(2,128)

intrapair = IntraPairVarianceLoss()
lgsftmloss = LargeMarginSoftmaxLoss(2,128)
normsftmxloss = NormalizedSoftmaxLoss(2,128)
proxyanchor = ProxyAnchorLoss(2,128)

msminer = MultiSimilarityMiner()
angularminer = AngularMiner()
hdcminer = HDCMiner()
behminer = BatchEasyHardMiner()


losses = [msloss, angularloss, ntxentloss, fastaploss, cosfaceloss]

miners = [msminer, angularminer, hdcminer, behminer]

lossnames = ["msloss", "angularloss", "ntxentloss", "fastaploss", "cosfaceloss", "multiloss"]
minernames = ["msminer", "angularminer", "hdcminer", "behminer"]

test_data2_loader_bs128.num_workers = 0
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
for lossname, loss_fn in zip(lossnames, losses):
    for minername, miner in zip(minernames, miners):
        if lossname == "ntxentloss" and minername == "angularminer":
            continue

        if lossname == "angularloss" and minername == "msminer":
            continue

        print(f"Loss: {lossname}, Miner: {minername}")
        print('===================================')



        encoder = TabularEncoder(category_sizes= category_sizes, \
                num_numeric = num_numeric, \
                hidden_dim = 512, \
                n_blocks = 3, \
                out_dim = 128, \
                dropout = 0.15, \
                emb_dim = 32)

        scl = SCLModel(encoder, proj_dim=128)
        classifier = ClassifierHead(in_dim=128, hidden=256, dropout=0.2)

        scl.load_state_dict(torch.load(f"/content/drive/MyDrive/QoT/scl_model_CB_128_{lossname}_{minername}_ft.pt"))
        classifier.load_state_dict(torch.load(f"/content/drive/MyDrive/QoT/mlp_model_CB_128_{lossname}_{minername}_ft.pt"))

        # ##EVAL BEFORE FT
        scl.to(device)
        scl.eval()
        classifier.to(device)
        classifier.eval()
        all_embeddings = []
        all_labels = []

        with torch.no_grad():
            for x_num, x_cat, batch_labels in test_data2_loader_bs256 :
                x_num = x_num.to(device)
                x_cat = x_cat.to(device)
                batch_labels = batch_labels.to(device)

                embeddings, _ = scl((x_num, x_cat))
                all_embeddings.append(embeddings.cpu())
                all_labels.append(batch_labels.cpu())

        X_data2_embeddings = torch.cat(all_embeddings)
        y_data2_true = torch.cat(all_labels)

        y_pred_classes = []
        with torch.no_grad():
            for i in range(0, len(X_data2_embeddings), 128):
                batch_embeddings = X_data2_embeddings[i:i+128].to(device)
                preds = classifier(batch_embeddings)
                pred_classes = torch.argmax(preds, dim=1)
                y_pred_classes.append(pred_classes.cpu())

        y_pred_classes = torch.cat(y_pred_classes)

        y_true = y_data2_true.numpy()
        y_pred = y_pred_classes.numpy()

        acc = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        roc = roc_auc_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)

        print("ðŸ“Š Evaluation Results on Data2:")
        print(f"Accuracy      : {acc:.4f}")
        print(f"F1-Score      : {f1:.4f}")
        print(f"Precision     : {precision:.4f}")
        print(f"Recall        : {recall:.4f}")
        print(f"ROC-AUC Score : {roc:.4f}")

        print("\nðŸ“„ Detailed Classification Report:")
        print(classification_report(y_true, y_pred, digits=4))

"""# TRAIN SCL, AND CLASSIFIER, JOINTLY"""

class SCLModelWithClassifier(nn.Module):
    def __init__(self, encoder: TabularEncoder, proj_dim=128, num_classes=2):
        super().__init__()
        self.encoder = encoder
        self.proj = ProjectionHead(encoder.out_bn.num_features, proj_dim)
        self.classifier = nn.Sequential(
            nn.Linear(encoder.out_bn.num_features, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x_num, x_cat = x
        emb = self.encoder(x_num, x_cat)       # normalized embedding
        proj = self.proj(emb)                  # for contrastive loss
        logits = self.classifier(emb)          # for CE loss
        return proj, emb, logits

import torch
from torch.cuda.amp import autocast, GradScaler
from tqdm.notebook import tqdm

class Trainer2:
    def __init__(self, sclmodel: SCLModelWithClassifier, train_loader, val_loader, loss_fn, miner_fn=None, optimizer=None, scheduler=None, device=None, use_amp=True, save_path="best_model.pt", early_stopping_patience=10):
        self.sclmodel = sclmodel
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.loss_fn = loss_fn
        self.miner_fn = miner_fn
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.use_amp = use_amp  # Automatic Mixed Precision toggle
        self.scaler = GradScaler() if use_amp else None  # GradScaler for AMP
        self.save_path = save_path
        self.early_stopping_patience = early_stopping_patience


        if self.sclmodel is not None:
            self.sclmodel.to(self.device)



    def train(self, epochs=10, validate_every=1):
        train_losses, val_losses = [], []
        best_val_loss = float('inf')
        epochs_no_improve = 0

        cls_loss = nn.CrossEntropyLoss()

        print(f"\nðŸ”µ Starting Training for {epochs} epochs...")
        print(f"ðŸ”µ Encoder: {'Yes' if self.sclmodel is not None else 'No'} | Miner: {'Yes' if self.miner_fn is not None else 'No'}")
        print(f"ðŸ”µ Device: {self.device} | Mixed Precision: {'Yes' if self.use_amp else 'No'}\n")

        for epoch in range(epochs):
            if self.sclmodel is not None:
                self.sclmodel.train()
            epoch_loss = 0
            num_batches = 0

            loop = tqdm(self.train_loader, desc=f"ðŸ”„ Epoch [{epoch+1}/{epochs}] Training", leave=False)
            for x_num, x_cat, label in loop:
                x_num = x_num.to(self.device, non_blocking=True)
                x_cat = x_cat.to(self.device, non_blocking=True)
                batch_labels = label.to(self.device, non_blocking=True)

                self.optimizer.zero_grad()

                if self.sclmodel is not None:
                    embeddings, _, logits = self.sclmodel((x_num, x_cat))
                else:
                    embeddings = (x_num, x_cat)

                if self.miner_fn:
                    pairs = self.miner_fn(embeddings, batch_labels)
                    cont_loss = self.loss_fn(embeddings, batch_labels, pairs)
                else:
                    cont_loss = self.loss_fn(embeddings, batch_labels)


                ce_loss = cls_loss(logits, batch_labels)
                a = 0.5
                b = 0.5
                loss = a*cont_loss + b*ce_loss

                # --- Backward ---
                if self.use_amp:
                    self.scaler.scale(loss).backward()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    self.optimizer.step()

                epoch_loss += loss.item()
                num_batches += 1

                loop.set_postfix(loss=loss.item())

            avg_epoch_loss = epoch_loss / num_batches
            avg_train_loss = epoch_loss / len(self.train_loader)
            train_losses.append(avg_epoch_loss)
            print(f"Epoch {epoch+1} | Contrastive: {cont_loss.item():.4f} | CE: {ce_loss.item():.4f} | Total: {loss.item():.4f}")


            current_lr = self.optimizer.param_groups[0]["lr"]
            print(f"âœ… Epoch {epoch+1}: LR = {current_lr:.6f}")

            if self.val_loader is not None:
              # --- Validation ---
              if (epoch + 1) % validate_every == 0:
                  val_loss = self.validate()
                  val_losses.append(val_loss)
                  print(f"ðŸ“ˆ Epoch [{epoch+1}/{epochs}] | Validation Loss: {val_loss:.6f}")

                  if val_loss < best_val_loss:
                      best_val_loss = val_loss
                      epochs_no_improve = 0
                      torch.save(self.sclmodel.state_dict(), self.save_path)
                      print(f"ðŸ’¾ Saved best model to {self.save_path}")
                  else:
                      epochs_no_improve += 1
                      print(f"â³ No improvement for {epochs_no_improve} epoch(s)")


                  if self.scheduler:
                      if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                          self.scheduler.step(val_loss)  # pass validation loss
                      else:
                          self.scheduler.step()


                  if epochs_no_improve >= self.early_stopping_patience:
                      print("â›” Early stopping triggered")
                      break

        if self.val_loader == None:
            torch.save(self.sclmodel.state_dict(), self.save_path)
            print(f"ðŸ’¾ Saved best model to {self.save_path}")
        return train_losses, val_losses

    def validate(self):
        if self.sclmodel is not None:
            self.sclmodel.eval()

        total_loss = 0
        num_batches = 0

        cls_loss = nn.CrossEntropyLoss()

        with torch.no_grad():
            loop = tqdm(self.val_loader, desc="ðŸ§ª Validation", leave=False)
            for x_num, x_cat, label in loop:
                x_num = x_num.to(self.device, non_blocking=True)
                x_cat = x_cat.to(self.device, non_blocking=True)
                batch_labels = label.to(self.device, non_blocking=True)

                with autocast(enabled=self.use_amp):
                    if self.sclmodel is not None:
                        embeddings, _, logits = self.sclmodel((x_num, x_cat))
                    else:
                        embeddings = (x_num, x_cat)

                    cont_loss = self.loss_fn(embeddings, batch_labels)
                    ce_loss = cls_loss(logits, batch_labels)
                    a = 0.5
                    b = 0.5
                    loss = a*cont_loss + b*ce_loss

                total_loss += loss.item()
                num_batches += 1

        avg_loss = total_loss / num_batches
        return avg_loss

from pytorch_metric_learning import regularizers
regl = regularizers.RegularFaceRegularizer()


msloss = MultiSimilarityLoss()
angularloss = AngularLoss()
ntxentloss = NTXentLoss()
fastaploss = FastAPLoss()
proxyanchor = ProxyAnchorLoss(2,128, weight_regularizer=regl)
cosfaceloss = CosFaceLoss(2,128, weight_regularizer=regl)

intrapair = IntraPairVarianceLoss()
lgsftmloss = LargeMarginSoftmaxLoss(2,128)
normsftmxloss = NormalizedSoftmaxLoss(2,128)


msminer = MultiSimilarityMiner()
angularminer = AngularMiner()
hdcminer = HDCMiner()
behminer = BatchEasyHardMiner()

#msloss, angularloss, ntxentloss, fastaploss,
losses = [ cosfaceloss, proxyanchor]

miners = [msminer, angularminer, hdcminer, behminer]
# "msloss", "angularloss", "ntxentloss", "fastaploss",
lossnames = [ "cosfaceloss", "proxyanchor"]
minernames = ["msminer", "angularminer", "hdcminer", "behminer"]

for lossname, loss_fn in zip(lossnames, losses):
    for minername, miner in zip(minernames, miners):
        print(f"Loss: {lossname}, Miner: {minername}")
        print('===================================')

        # if lossname in ["msloss", "angularloss", "ntxentloss" , "fastaploss"]:
        #   continue

        if lossname=="ntxentloss" and minername=="angularminer":
            continue
        if lossname=="cosfaceloss" and minername=="msminer":
            continue

        encoder = TabularEncoder(category_sizes= category_sizes, \
                num_numeric = num_numeric, \
                hidden_dim = 512, \
                n_blocks = 3, \
                out_dim = 128, \
                dropout = 0.15, \
                emb_dim = 32)

        scl = SCLModelWithClassifier(encoder, proj_dim=128)

        optimizer = optim.AdamW(scl.parameters(), lr=1e-3)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')

        trainer = Trainer2(
            sclmodel=scl,
            train_loader=train_loader_cb_bs128,
            val_loader=val_loader_bs128,
            loss_fn=loss_fn,
            miner_fn=miner,
            optimizer=optimizer,
            scheduler=scheduler,
            use_amp=False,
            save_path=f"/content/drive/MyDrive/QoT/joint_scl_model_CB_128_{lossname}_{minername}.pt",
            early_stopping_patience=3
        )


        train_losses, val_losses = trainer.train(epochs=50, validate_every=3)

        print(" ############ TRAINING DONE ############ ")

msloss = MultiSimilarityLoss()
angularloss = AngularLoss()
ntxentloss = NTXentLoss()
fastaploss = FastAPLoss()
cosfaceloss = CosFaceLoss(2,128)

intrapair = IntraPairVarianceLoss()
lgsftmloss = LargeMarginSoftmaxLoss(2,128)
normsftmxloss = NormalizedSoftmaxLoss(2,128)
proxyanchor = ProxyAnchorLoss(2,128)

msminer = MultiSimilarityMiner()
angularminer = AngularMiner()
hdcminer = HDCMiner()
behminer = BatchEasyHardMiner()


losses = [msloss, angularloss, ntxentloss, fastaploss, cosfaceloss]

miners = [msminer, angularminer, hdcminer, behminer]

lossnames = ["msloss", "angularloss", "ntxentloss", "fastaploss", "cosfaceloss"]
minernames = ["msminer", "angularminer", "hdcminer", "behminer"]

test_data2_loader_bs128.num_workers = 0
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
for lossname, loss_fn in zip(lossnames, losses):
    for minername, miner in zip(minernames, miners):
        if lossname == "ntxentloss" and minername == "angularminer":
            continue

        print(f"Loss: {lossname}, Miner: {minername}")
        print('===================================')



        encoder = TabularEncoder(category_sizes= category_sizes, \
                num_numeric = num_numeric, \
                hidden_dim = 512, \
                n_blocks = 3, \
                out_dim = 128, \
                dropout = 0.15, \
                emb_dim = 32)

        scl = SCLModelWithClassifier(encoder, proj_dim=128)

        scl.load_state_dict(torch.load(f"/content/drive/MyDrive/QoT/joint_scl_model_CB_128_{lossname}_{minername}.pt"))

        # ##EVAL BEFORE FT
        scl.to(device)
        scl.eval()

        all_logits, all_labels = [], []

        with torch.no_grad():
            for x_num, x_cat, batch_labels in test_data2_loader_bs128 :
                x_num = x_num.to(device)
                x_cat = x_cat.to(device)
                batch_labels = batch_labels.to(device)

                _, _, logits = scl((x_num, x_cat))
                all_logits.append(logits.cpu())
                all_labels.append(batch_labels.cpu())

        logits = torch.cat(all_logits)          # (N, 2)
        y_true = torch.cat(all_labels).numpy()  # (N,)

        # 3.3  Hard predictions
        y_pred = torch.argmax(logits, dim=1).numpy()

        # 3.4  Probability for ROCâ€‘AUC (column 1 = P(class=1))
        y_proba = torch.softmax(logits, dim=1)[:, 1].numpy()

        # 3.5  Metrics
        acc       = accuracy_score(y_true, y_pred)
        f1        = f1_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall    = recall_score(y_true, y_pred)
        roc       = roc_auc_score(y_true, y_proba)

        # 3.6  Report
        print("ðŸ“Š Evaluation on Dataâ€‘2")
        print(f"Accuracy      : {acc:.4f}")
        print(f"F1â€‘score      : {f1:.4f}")
        print(f"Precision     : {precision:.4f}")
        print(f"Recall        : {recall:.4f}")
        print(f"ROCâ€‘AUC       : {roc:.4f}\n")
        print("Classification report:")
        print(classification_report(y_true, y_pred, digits=4))

msloss = MultiSimilarityLoss()
angularloss = AngularLoss()
ntxentloss = NTXentLoss()
fastaploss = FastAPLoss()
cosfaceloss = CosFaceLoss(2,128)

intrapair = IntraPairVarianceLoss()
lgsftmloss = LargeMarginSoftmaxLoss(2,128)
normsftmxloss = NormalizedSoftmaxLoss(2,128)
proxyanchor = ProxyAnchorLoss(2,128)

msminer = MultiSimilarityMiner()
angularminer = AngularMiner()
hdcminer = HDCMiner()
behminer = BatchEasyHardMiner()


losses = [msloss, angularloss, ntxentloss, fastaploss, cosfaceloss]

miners = [msminer, angularminer, hdcminer, behminer]

lossnames = ["msloss", "angularloss", "ntxentloss", "fastaploss", "cosfaceloss"]
minernames = ["msminer", "angularminer", "hdcminer", "behminer"]

test_data2_loader_bs128.num_workers = 0
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
for lossname, loss_fn in zip(lossnames, losses):
    for minername, miner in zip(minernames, miners):
        if lossname == "ntxentloss" and minername == "angularminer":
            continue

        if lossname != "cosfaceloss":
            continue
        if lossname == "cosfaceloss" and minername == "msminer":
            continue


        print(f"Loss: {lossname}, Miner: {minername}")
        print('===================================')



        encoder = TabularEncoder(category_sizes= category_sizes, \
                num_numeric = num_numeric, \
                hidden_dim = 512, \
                n_blocks = 3, \
                out_dim = 128, \
                dropout = 0.15, \
                emb_dim = 32)

        scl = SCLModelWithClassifier(encoder, proj_dim=128)

        scl.load_state_dict(torch.load(f"/content/drive/MyDrive/QoT/joint_scl_model_CB_128_{lossname}_{minername}.pt"))

        # ##EVAL BEFORE FT
        scl.to(device)
        scl.eval()

        all_logits, all_labels = [], []

        with torch.no_grad():
            for x_num, x_cat, batch_labels in test_data2_loader_bs128 :
                x_num = x_num.to(device)
                x_cat = x_cat.to(device)
                batch_labels = batch_labels.to(device)

                _, _, logits = scl((x_num, x_cat))
                all_logits.append(logits.cpu())
                all_labels.append(batch_labels.cpu())

        logits = torch.cat(all_logits)          # (N, 2)
        y_true = torch.cat(all_labels).numpy()  # (N,)

        # 3.3  Hard predictions
        y_pred = torch.argmax(logits, dim=1).numpy()

        # 3.4  Probability for ROCâ€‘AUC (column 1 = P(class=1))
        y_proba = torch.softmax(logits, dim=1)[:, 1].numpy()

        # 3.5  Metrics
        acc       = accuracy_score(y_true, y_pred)
        f1        = f1_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall    = recall_score(y_true, y_pred)
        roc       = roc_auc_score(y_true, y_proba)

        # 3.6  Report
        print("ðŸ“Š Evaluation on Dataâ€‘2")
        print(f"Accuracy      : {acc:.4f}")
        print(f"F1â€‘score      : {f1:.4f}")
        print(f"Precision     : {precision:.4f}")
        print(f"Recall        : {recall:.4f}")
        print(f"ROCâ€‘AUC       : {roc:.4f}\n")
        print("Classification report:")
        print(classification_report(y_true, y_pred, digits=4))

from pytorch_metric_learning import regularizers
regl = regularizers.RegularFaceRegularizer()


msloss = MultiSimilarityLoss()
angularloss = AngularLoss()
ntxentloss = NTXentLoss()
fastaploss = FastAPLoss()
proxyanchor = ProxyAnchorLoss(2,128, weight_regularizer=regl)
cosfaceloss = CosFaceLoss(2,128, weight_regularizer=regl)

intrapair = IntraPairVarianceLoss()
lgsftmloss = LargeMarginSoftmaxLoss(2,128)
normsftmxloss = NormalizedSoftmaxLoss(2,128)


msminer = MultiSimilarityMiner()
angularminer = AngularMiner()
hdcminer = HDCMiner()
behminer = BatchEasyHardMiner()

#msloss, angularloss, ntxentloss, fastaploss,
losses = [ cosfaceloss, proxyanchor]

miners = [msminer, angularminer, hdcminer, behminer]
# "msloss", "angularloss", "ntxentloss", "fastaploss",
lossnames = [ "cosfaceloss", "proxyanchor"]
minernames = ["msminer", "angularminer", "hdcminer", "behminer"]

test_data2_loader_bs128.num_workers = 0
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
for lossname, loss_fn in zip(lossnames, losses):
    for minername, miner in zip(minernames, miners):
        if lossname == "proxyanchor" and minername == "behminer":
            continue

        print(f"Loss: {lossname}, Miner: {minername}")
        print('===================================')



        encoder = TabularEncoder(category_sizes= category_sizes, \
                num_numeric = num_numeric, \
                hidden_dim = 512, \
                n_blocks = 3, \
                out_dim = 128, \
                dropout = 0.15, \
                emb_dim = 32)

        scl = SCLModelWithClassifier(encoder, proj_dim=128)

        scl.load_state_dict(torch.load(f"/content/drive/MyDrive/QoT/joint_scl_model_CB_128_{lossname}_{minername}.pt"))

        # ##EVAL BEFORE FT
        scl.to(device)
        scl.eval()

        all_logits, all_labels = [], []

        with torch.no_grad():
            for x_num, x_cat, batch_labels in test_data2_loader_bs128 :
                x_num = x_num.to(device)
                x_cat = x_cat.to(device)
                batch_labels = batch_labels.to(device)

                _, _, logits = scl((x_num, x_cat))
                all_logits.append(logits.cpu())
                all_labels.append(batch_labels.cpu())

        logits = torch.cat(all_logits)          # (N, 2)
        y_true = torch.cat(all_labels).numpy()  # (N,)

        # 3.3  Hard predictions
        y_pred = torch.argmax(logits, dim=1).numpy()

        # 3.4  Probability for ROCâ€‘AUC (column 1 = P(class=1))
        y_proba = torch.softmax(logits, dim=1)[:, 1].numpy()

        # 3.5  Metrics
        acc       = accuracy_score(y_true, y_pred)
        f1        = f1_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall    = recall_score(y_true, y_pred)
        roc       = roc_auc_score(y_true, y_proba)

        # 3.6  Report
        print("ðŸ“Š Evaluation on Dataâ€‘2")
        print(f"Accuracy      : {acc:.4f}")
        print(f"F1â€‘score      : {f1:.4f}")
        print(f"Precision     : {precision:.4f}")
        print(f"Recall        : {recall:.4f}")
        print(f"ROCâ€‘AUC       : {roc:.4f}\n")
        print("Classification report:")
        print(classification_report(y_true, y_pred, digits=4))

