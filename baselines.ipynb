{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9ff570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data1 = pd.read_csv('datasets/cleaned_lightpath_dataset.csv')\n",
    "target1 = pd.read_csv('datasets/cleaned_lightpath_target.csv')\n",
    "\n",
    "data2 = pd.read_csv('datasets/cleaned_lightpath_dataset_2.csv')\n",
    "target2 = pd.read_csv('datasets/cleaned_lightpath_target_2.csv')\n",
    "\n",
    "data_5 = pd.read_csv('datasets/data1_plus_5.csv')\n",
    "target_5 = pd.read_csv('datasets/target1_plus_5.csv')\n",
    "\n",
    "data_5_balanced = pd.read_csv('datasets/data1_plus_5_balanced.csv')\n",
    "target_5_balanced = pd.read_csv('datasets/target1_plus_5_balanced.csv')\n",
    "\n",
    "data_10 = pd.read_csv('datasets/data1_plus_10.csv')\n",
    "target_10 = pd.read_csv('datasets/target1_plus_10.csv')\n",
    "\n",
    "data_10_balanced = pd.read_csv('datasets/data1_plus_10_balanced.csv')\n",
    "target_10_balanced = pd.read_csv('datasets/target1_plus_10_balanced.csv')\n",
    "\n",
    "data15 = pd.read_csv('datasets/data1_plus_15.csv')\n",
    "target15 = pd.read_csv('datasets/target1_plus_15.csv')\n",
    "\n",
    "data15_balanced = pd.read_csv('datasets/data1_plus_15_balanced.csv')\n",
    "target15_balanced = pd.read_csv('datasets/target1_plus_15_balanced.csv')\n",
    "\n",
    "data20 = pd.read_csv('datasets/data1_plus_20.csv')\n",
    "target20 = pd.read_csv('datasets/target1_plus_20.csv')\n",
    "\n",
    "data20_balanced = pd.read_csv('datasets/data1_plus_20_balanced.csv')\n",
    "target20_balanced = pd.read_csv('datasets/target1_plus_20_balanced.csv')\n",
    "\n",
    "shard1 = pd.read_csv('datasets/dataset2_shard_1.csv')\n",
    "target_shard1 = pd.read_csv('datasets/target2_shard_1.csv')\n",
    "\n",
    "shard2 = pd.read_csv('datasets/dataset2_shard_2.csv')\n",
    "target_shard2 = pd.read_csv('datasets/target2_shard_2.csv')\n",
    "\n",
    "shard3 = pd.read_csv('datasets/dataset2_shard_3.csv')\n",
    "target_shard3 = pd.read_csv('datasets/target2_shard_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "missing = []\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception:\n",
    "    missing.append(\"xgboost\")\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except Exception:\n",
    "    missing.append(\"catboost\")\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except Exception:\n",
    "    missing.append(\"lightgbm\")\n",
    "\n",
    "if missing:\n",
    "    raise ImportError(\n",
    "        \"Missing packages: \" + \", \".join(missing) + \". Install them before running this cell.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d0d15",
   "metadata": {},
   "source": [
    "# Model-specific preprocessing and hyperparameter search\n",
    "This section defines lightweight search spaces per model and a helper to run a small randomized search. The goal is to compare preprocessing choices (e.g., scaling) and core hyperparameters without an exhaustive run.\n",
    "\n",
    "\n",
    "*Note*: This code (below) will help you to tune model on specific single data then return best outperforming model on the training data not on evals. Which is intuitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def build_search_spaces():\n",
    "    # Preprocessing choices: with/without scaling for linear models\n",
    "    preproc_options = {\n",
    "        \"scaled\": Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler(with_mean=False)),\n",
    "            ]\n",
    "        ),\n",
    "        \"unscaled\": Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    spaces = {\n",
    "        \"LogisticRegression\": {\n",
    "            \"model\": LogisticRegression(max_iter=5000, n_jobs=-1),\n",
    "            \"param_distributions\": {\n",
    "                \"preproc\": [\"scaled\"],\n",
    "                \"model__C\": [0.01, 0.1, 1, 10],\n",
    "                \"model__penalty\": [\"l2\"],\n",
    "                \"model__solver\": [\"lbfgs\", \"saga\"],\n",
    "            },\n",
    "        },\n",
    "        \"LogReg_L1\": {\n",
    "            \"model\": LogisticRegression(max_iter=5000, penalty=\"l1\", solver=\"saga\", n_jobs=-1),\n",
    "            \"param_distributions\": {\n",
    "                \"preproc\": [\"scaled\"],\n",
    "                \"model__C\": [0.01, 0.1, 1, 10],\n",
    "            },\n",
    "        },\n",
    "        \"LogReg_L2\": {\n",
    "            \"model\": LogisticRegression(max_iter=5000, penalty=\"l2\", solver=\"saga\", n_jobs=-1),\n",
    "            \"param_distributions\": {\n",
    "                \"preproc\": [\"scaled\"],\n",
    "                \"model__C\": [0.01, 0.1, 1, 10],\n",
    "            },\n",
    "        },\n",
    "        \"RandomForest\": {\n",
    "            \"model\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "            \"param_distributions\": {\n",
    "                \"preproc\": [\"unscaled\"],\n",
    "                \"model__n_estimators\": [200, 300, 500],\n",
    "                \"model__max_depth\": [None, 10, 20, 30],\n",
    "                \"model__min_samples_split\": [2, 5, 10],\n",
    "                \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            },\n",
    "        },\n",
    "        \"ExtraTrees\": {\n",
    "            \"model\": ExtraTreesClassifier(random_state=42, n_jobs=-1),\n",
    "            \"param_distributions\": {\n",
    "                \"preproc\": [\"unscaled\"],\n",
    "                \"model__n_estimators\": [200, 300, 500],\n",
    "                \"model__max_depth\": [None, 10, 20, 30],\n",
    "                \"model__min_samples_split\": [2, 5, 10],\n",
    "                \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            },\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBClassifier(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                eval_metric=\"logloss\",\n",
    "                n_jobs=-1,\n",
    "                random_state=42,\n",
    "            ),\n",
    "            \"param_distributions\": {\n",
    "                \"preproc\": [\"unscaled\"],\n",
    "                \"model__n_estimators\": [200, 300, 500],\n",
    "                \"model__max_depth\": [4, 6, 8],\n",
    "                \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "                \"model__subsample\": [0.7, 0.8, 1.0],\n",
    "                \"model__colsample_bytree\": [0.7, 0.8, 1.0],\n",
    "            },\n",
    "        },\n",
    "        \"LightGBM\": {\n",
    "            \"model\": LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=31, random_state=42, n_jobs=-1),\n",
    "            \"param_distributions\": {\n",
    "                \"preproc\": [\"unscaled\"],\n",
    "                \"model__n_estimators\": [200, 300, 500],\n",
    "                \"model__num_leaves\": [15, 31, 63],\n",
    "                \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "                \"model__subsample\": [0.7, 0.8, 1.0],\n",
    "                \"model__colsample_bytree\": [0.7, 0.8, 1.0],\n",
    "            },\n",
    "        },\n",
    "        \"CatBoost\": {\n",
    "            \"model\": CatBoostClassifier(iterations=300, learning_rate=0.1, depth=6, random_seed=42, verbose=False),\n",
    "            \"param_distributions\": {\n",
    "                \"preproc\": [\"unscaled\"],\n",
    "                \"model__iterations\": [200, 300, 500],\n",
    "                \"model__depth\": [4, 6, 8],\n",
    "                \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "                \"model__l2_leaf_reg\": [1, 3, 5, 7],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    return preproc_options, spaces\n",
    "\n",
    "def build_pipeline(preproc_name, preproc_options, model):\n",
    "    return Pipeline(steps=[(\"preproc\", preproc_options[preproc_name]), (\"model\", model)])\n",
    "\n",
    "def tune_model(X_train_df, y_train_df, model_name, n_iter=20, cv_splits=3, scoring=\"f1_weighted\", random_state=42):\n",
    "    X = pd.get_dummies(X_train_df, drop_first=False)\n",
    "    y = y_train_df.iloc[:, 0].values.ravel()\n",
    "\n",
    "    preproc_options, spaces = build_search_spaces()\n",
    "    if model_name not in spaces:\n",
    "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "    space = spaces[model_name]\n",
    "    base_model = space[\"model\"]\n",
    "    param_distributions = space[\"param_distributions\"]\n",
    "\n",
    "    def _make_estimator(preproc_choice):\n",
    "        return build_pipeline(preproc_choice, preproc_options, base_model)\n",
    "\n",
    "    # Wrap preproc choice as a parameter by using a custom estimator per candidate\n",
    "    estimators = []\n",
    "    for preproc_choice in param_distributions[\"preproc\"]:\n",
    "        estimators.append((_make_estimator(preproc_choice), preproc_choice))\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    best_result = None\n",
    "    for estimator, preproc_choice in estimators:\n",
    "        params = {k: v for k, v in param_distributions.items() if k != \"preproc\"}\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_distributions=params,\n",
    "            n_iter=n_iter,\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            verbose=0,\n",
    "        )\n",
    "        search.fit(X, y)\n",
    "        result = {\n",
    "            \"model\": model_name,\n",
    "            \"preproc\": preproc_choice,\n",
    "            \"best_score\": search.best_score_,\n",
    "            \"best_params\": search.best_params_,\n",
    "        }\n",
    "        if best_result is None or result[\"best_score\"] > best_result[\"best_score\"]:\n",
    "            best_result = result\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2d77a",
   "metadata": {},
   "source": [
    "## Example: tune one model on one dataset\n",
    "Adjust `model_name` and `n_iter` as needed. Start small to keep runtime reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46964ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: tune LogisticRegression on data_10 (change as needed)\n",
    "best_lr = tune_model(data_10, target_10, model_name=\"LogisticRegression\", n_iter=15, cv_splits=3)\n",
    "best_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349e3e2",
   "metadata": {},
   "source": [
    "_______________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec438a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Datasets (train) ---\n",
    "train_datasets = [\n",
    "    (\"data1\", data1, target1),\n",
    "    (\"data_5\", data_5, target_5),\n",
    "    (\"data_5_balanced\", data_5_balanced, target_5_balanced),\n",
    "    (\"data_10\", data_10, target_10),\n",
    "    (\"data_10_balanced\", data_10_balanced, target_10_balanced),\n",
    "    (\"data_15\", data15, target15),\n",
    "    (\"data_15_balanced\", data15_balanced, target15_balanced),\n",
    "    (\"data_20\", data20, target20),\n",
    "    (\"data_20_balanced\", data20_balanced, target20_balanced),\n",
    "]\n",
    "\n",
    "# --- Datasets (evaluation) ---\n",
    "eval_datasets = [\n",
    "    (\"data2\", data2, target2),\n",
    "    (\"shard1\", shard1, target_shard1),\n",
    "    (\"shard2\", shard2, target_shard2),\n",
    "    (\"shard3\", shard3, target_shard3),\n",
    "]\n",
    "\n",
    "# --- Models (basic settings) ---\n",
    "models = [\n",
    "    (\"LogisticRegression\", make_pipeline(StandardScaler(with_mean=False), LogisticRegression(max_iter=2000, n_jobs=-1))),\n",
    "    (\"LogReg_L1\", make_pipeline(StandardScaler(with_mean=False), LogisticRegression(max_iter=5000, penalty=\"l1\", solver=\"saga\", n_jobs=-1))),\n",
    "    (\"LogReg_L2\", make_pipeline(StandardScaler(with_mean=False), LogisticRegression(max_iter=5000, penalty=\"l2\", solver=\"saga\", n_jobs=-1))),\n",
    "    (\"XGBoost\", XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8, eval_metric=\"logloss\", n_jobs=-1, random_state=42)),\n",
    "    (\"CatBoost\", CatBoostClassifier(iterations=300, learning_rate=0.1, depth=6, random_seed=42, verbose=False)),\n",
    "    (\"LightGBM\", LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=31, random_state=42, n_jobs=-1)),\n",
    "    (\"RandomForest\", RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)),\n",
    "    (\"ExtraTrees\", ExtraTreesClassifier(n_estimators=300, random_state=42, n_jobs=-1)),\n",
    "]\n",
    "\n",
    "# Map 1 model to 1 dataset (cycles through models if datasets > models)\n",
    "model_cycle = cycle(models)\n",
    "training_plan = [(ds_name, X, y, *next(model_cycle)) for ds_name, X, y in train_datasets]\n",
    "\n",
    "def _prepare_xy(X_df: pd.DataFrame, y_df: pd.DataFrame):\n",
    "    X = pd.get_dummies(X_df, drop_first=False)\n",
    "    y = y_df.iloc[:, 0].values.ravel()\n",
    "    return X, y\n",
    "\n",
    "def _align_eval_columns(X_eval: pd.DataFrame, train_columns):\n",
    "    X_eval = pd.get_dummies(X_eval, drop_first=False)\n",
    "    X_eval = X_eval.reindex(columns=train_columns, fill_value=0)\n",
    "    return X_eval\n",
    "\n",
    "def _get_score(model, X_eval):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X_eval)\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return model.decision_function(X_eval)\n",
    "    return model.predict(X_eval)\n",
    "\n",
    "results = []\n",
    "\n",
    "for train_name, X_train_df, y_train_df, model_name, model in training_plan:\n",
    "    X_train, y_train = _prepare_xy(X_train_df, y_train_df)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_columns = X_train.columns\n",
    "    for eval_name, X_eval_df, y_eval_df in eval_datasets:\n",
    "        X_eval = _align_eval_columns(X_eval_df, train_columns)\n",
    "        y_eval = y_eval_df.iloc[:, 0].values.ravel()\n",
    "        y_pred = model.predict(X_eval)\n",
    "        report = classification_report(y_eval, y_pred, output_dict=True, zero_division=0)\n",
    "        precision = report[\"weighted avg\"][\"precision\"]\n",
    "        recall = report[\"weighted avg\"][\"recall\"]\n",
    "        f1 = report[\"weighted avg\"][\"f1-score\"]\n",
    "        accuracy = report[\"accuracy\"]\n",
    "        try:\n",
    "            pr_auc = average_precision_score(y_eval, _get_score(model, X_eval), average=\"weighted\")\n",
    "        except Exception:\n",
    "            pr_auc = np.nan\n",
    "        results.append({\n",
    "            \"train_dataset\": train_name,\n",
    "            \"model\": model_name,\n",
    "            \"eval_dataset\": eval_name,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"pr_auc\": pr_auc,\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
